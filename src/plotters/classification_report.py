
from enum import unique
import os
from random import sample
import re
import pickle
import numpy as np
import pandas as pd

import plotly.express as px
from matplotlib import pyplot as plt
from crossvalidation.scorers.binary_scorer import BinaryClfScorer
from crossvalidation.scorers.fairness_binary_scorer import BinaryFairnessScorer

from plotters.report import Report
from collections import Counter

# import dash_core_components as dcc
from plotly.offline import plot
from plotly.subplots import make_subplots
import plotly.graph_objects as go

class ClassificationReport(Report):
    def __init__(self, settings:dict):
        self._settings = dict(settings)
        self.get_reporting_path('data_reports')
        self._colours = [
            '#0C1618', '#004346', '#3F7267', '#7D9D8B', '#FAF4D3', '#E6D06A', '#DCBE35', '#D1AC00', '#7389AE',
            '#001219', '#005F73', '#0A9396', '#E9D8A6', '#EE9B00', '#CA6702', '#BB3E03', '#AE2012', '#9B2226'
        ]

# General
    def _crawl(self):
        # crawl paths
        xval_path = []
        experiment_path = '../experiments/' + self._settings['experiment']['name'] + '/'
        for (dirpath, _, filenames) in os.walk(experiment_path):
            files = [os.path.join(dirpath, file) for file in filenames]
            xval_path.extend(files)
        xval_path = [xval for xval in xval_path if '/results/' in xval and 'config.pkl' not in xval and '.csv' not in xval]
        xval_path = [xval for xval in xval_path if 'exclude' not in xval and '.DS_Store' not in xval]
        
        # Load results
        key_re = re.compile('{}[A-z\_\-0-9/]*mitigation([A-z\_\-0-9/]*)/2025'.format(experiment_path))
        xvs = {}
        for xv in xval_path:
            if 'baseline' in xv:
                curr_key = 'baseline'
            else:
                key = key_re.findall(xv)
                print('processing: {} ({}) '.format(xv, key))
                assert len(key) == 1
                curr_key = '/'.join(key[0].split('/')[:self._settings['plotter']['experiment_indices']])
                curr_key = curr_key.replace('in', '').replace('post', '').replace('pre', '')
            xvs[curr_key] = {}

            # cross validation
            with open(xv, 'rb') as fp:
                xvs[curr_key]['data'] = pickle.load(fp)

            # config file
            conpath = '{}/results/config.pkl'.format(xv.split('/results/')[0])
            with open(conpath, 'rb') as fp:
                xvs[curr_key]['config'] = pickle.load(fp)

            # result path
            xvs[curr_key]['result_path'] = conpath.replace('config.pkl', 'overall_results/')
            self._result_path = conpath.replace('config.pkl', '/statistics/')
            os.makedirs(xvs[curr_key]['result_path'], exist_ok=True)
            os.makedirs(self._result_path, exist_ok=True)

        return xvs

    def extract_metrics_perfold(self, experiment, labels, demographics):
        """Computes all metrics, for each fold

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        self._nfolds = 10

        # Collect scores
        binary_scorer = BinaryClfScorer(self._settings)
        fairness_scorer = BinaryFairnessScorer(self._settings)
        scores = {}
        for fold in range(10):
            truths = [labels[tidx] for tidx in experiment[fold]['test_index']]
            probas = experiment[fold]['y_proba']
            preds = experiment[fold]['y_pred']
            demos = [demographics[tidx] for tidx in experiment[fold]['test_index']] 
            scores[fold] =  {
                'classification': binary_scorer.get_scores(truths, preds, probas, demos),
                'fairness': fairness_scorer.get_fairness_scores(truths, preds, probas, demos)
            }
            
        # Format fairness scores
        fairness_scores = {}
        for attribute in self._settings['fairness']['attributes']:
            fairness_scores[attribute] = {}
            for fairness_metric in fairness_scorer.get_metrics():
                fairness_scores[attribute][fairness_metric] = {}
                if fairness_metric != 'equalised_odds':
                    for demographic_attribute in scores[0]['fairness'][attribute][fairness_metric]:
                        fairness_scores[attribute][fairness_metric][demographic_attribute] = []
                        for fold in range(self._nfolds):
                            if demographic_attribute in scores[fold]['fairness'][attribute][fairness_metric]:
                                fairness_scores[attribute][fairness_metric][demographic_attribute].append(
                                    scores[fold]['fairness'][attribute][fairness_metric][demographic_attribute]
                                )
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        fairness_scores[attribute][fairness_metric][submetric] = {}
                        for demographic_attribute in scores[0]['fairness'][attribute][fairness_metric][submetric]:
                            fairness_scores[attribute][fairness_metric][submetric][demographic_attribute] = []
                            for fold in range(self._nfolds):
                                if demographic_attribute in scores[fold]['fairness'][attribute][fairness_metric][submetric]:
                                    fairness_scores[attribute][fairness_metric][submetric][demographic_attribute].append(
                                        scores[fold]['fairness'][attribute][fairness_metric][submetric][demographic_attribute]
                                    ) 

        # Format classification scores
        classification_scores = {}
        for clf_metric in self._settings['crossvalidation']['scorer']['scoring_metrics']:
            classification_scores[clf_metric] = []
            for fold in range(10):
                classification_scores[clf_metric].append(
                    scores[fold]['classification'][clf_metric]
                )

        return fairness_scores, classification_scores

    def extract_metrics_bootstrap(self, experiment, labels, demographics):
        """Computes all metrics as a bootstrapping over all predictions over all folds

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        self._nfolds = self._settings['n_bootstraps']
        
        # Prepare the data
        truths = []
        probas = []
        preds = []
        demos = []
        for fold in range(10):
            if fold in experiment:
                truths = [*truths, *[labels[tidx] for tidx in experiment[fold]['test_index']]]
                probas = [*probas, *experiment[fold]['y_proba']]
                preds = [*preds, *experiment[fold]['y_pred']]
                demos = [*demos, *[demographics[tidx] for tidx in experiment[fold]['test_index']]]
        assert len(truths) == len(probas) and len(probas) == len(preds) and len(preds) == len(demos)
        # Create the bootstrap sample
        binary_scorer = BinaryClfScorer(self._settings)
        fairness_scorer = BinaryFairnessScorer(self._settings)
        n_instances = len(truths)
        sample_indices = [i for i in range(n_instances)]
        fairness_boots = []
        binary_boots = []
        for _ in range(self._nfolds):
            boot_sample = np.random.choice(sample_indices, size=n_instances, replace=True)
            boot_truths = [truths[bidx] for bidx in boot_sample]
            boot_probas = [probas[bidx] for bidx in boot_sample]
            boot_preds = [preds[bidx] for bidx in boot_sample]
            boot_demos = [demos[bidx] for bidx in boot_sample]

            fairness_boots.append(fairness_scorer.get_fairness_scores(boot_truths, boot_preds, boot_probas, boot_demos))
            binary_boots.append(binary_scorer.get_scores(boot_truths, boot_preds, boot_probas, boot_demos))

        # Format fairness scores
        fairness_scores = {}
        for attribute in self._settings['fairness']['attributes']:
            fairness_scores[attribute] = {}
            for fairness_metric in fairness_scorer.get_metrics():
                fairness_scores[attribute][fairness_metric] = {}
                if fairness_metric != 'equalised_odds':
                    for demographic_attribute in fairness_boots[0][attribute][fairness_metric]:
                        fairness_scores[attribute][fairness_metric][demographic_attribute] = []
                        for fold in range(self._nfolds):
                            if demographic_attribute in fairness_boots[fold][attribute][fairness_metric]:
                                fairness_scores[attribute][fairness_metric][demographic_attribute].append(
                                    fairness_boots[fold][attribute][fairness_metric][demographic_attribute]
                                )
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        fairness_scores[attribute][fairness_metric][submetric] = {}
                        for demographic_attribute in fairness_boots[0][attribute][fairness_metric][submetric]:
                            fairness_scores[attribute][fairness_metric][submetric][demographic_attribute] = []
                            for fold in range(self._nfolds):
                                if demographic_attribute in fairness_boots[fold][attribute][fairness_metric][submetric]:
                                    fairness_scores[attribute][fairness_metric][submetric][demographic_attribute].append(
                                        fairness_boots[fold][attribute][fairness_metric][submetric][demographic_attribute]
                                    )

        # Format classification scores
        classification_scores = {}
        for clf_metric in self._settings['crossvalidation']['scorer']['scoring_metrics']:
            classification_scores[clf_metric] = []
            for fold in range(self._nfolds):
                classification_scores[clf_metric].append(
                    binary_boots[fold][clf_metric]
                )

        return fairness_scores, classification_scores

    def extract_predictions(self, experiment, labels, demographics):
        """Computes all metrics as a bootstrapping over all predictions over all folds

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        # Prepare the data
        truths = []
        probas = []
        preds = []
        demos = []
        for fold in range(10):
            truths = [*truths, *[labels[tidx] for tidx in experiment[fold]['test_index']]]
            probas = [*probas, *experiment[fold]['y_proba']]
            preds = [*preds, *experiment[fold]['y_pred']]
            demos = [*demos, *[demographics[tidx] for tidx in experiment[fold]['test_index']]]
        assert len(truths) == len(probas) and len(probas) == len(preds) and len(preds) == len(demos)
        return {'truths': truths, 'probas': probas, 'preds': preds, 'demos': demos}

    def _merge_experiment_scores(self, fairness_scores, classification_scores):
        """Merge the experiment metrics together

        Args:
            fairness_scores (dict): keys are the experiment, with fairness_scores as returned by extract_metrics
            classification_scores (dict): keys are the experiment, with the extract_metrics as returned by extract_metrics

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                    - experiment 
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        new_fairness_scores = {}
        new_classification_scores = {}
        experiment_names = [name for name in fairness_scores.keys()]

        for attribute in fairness_scores[experiment_names[0]]:
            new_fairness_scores[attribute] = {}
            for metric in fairness_scores[experiment_names[0]][attribute]:
                new_fairness_scores[attribute][metric] = {}
                if metric != 'equalised_odds':
                    for experiment in experiment_names:
                        new_fairness_scores[attribute][metric][experiment] = {}
                        for demographic in fairness_scores[experiment][attribute][metric]:
                            new_fairness_scores[attribute][metric][experiment][demographic] = fairness_scores[experiment][attribute][metric][demographic]
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        new_fairness_scores[attribute][metric][submetric] = {}
                        for experiment in experiment_names:
                            new_fairness_scores[attribute][metric][submetric][experiment] = {}
                            for demographic in fairness_scores[experiment][attribute][metric][submetric]:
                                new_fairness_scores[attribute][metric][submetric][experiment][demographic] = fairness_scores[experiment][attribute][metric][submetric][demographic]

        for metric in classification_scores[experiment_names[0]]:
            new_classification_scores[metric] = {}
            for experiment in experiment_names:
                new_classification_scores[metric][experiment] = classification_scores[experiment][metric]

        return new_fairness_scores, new_classification_scores

# Plotting
    def _format_fairness(self, fairness_scores):
        """format the fairness scores dictionary for plotting mode
        """
        fairness_df = []
        for experiment in fairness_scores:
            for demo in fairness_scores[experiment]:
                fdf = pd.DataFrame.from_dict(fairness_scores[experiment][demo]).reset_index()
                fdf = fdf.rename(columns={'index': 'fold', 0: 'scores'})
                fdf['demographic'] = demo
                fdf['experiment'] = experiment
                fairness_df.append(fdf)
        fairness_df = pd.concat(fairness_df)
        fairness_df.dropna(how='any')
        fairness_df = fairness_df[['scores', 'demographic', 'experiment']]
        return fairness_df

    def plot_fairness(self, fairness_scores):
        """creates one plot per attribute, one row per metric, one x point per experiment, one colour per demographic attribute, and y on the scores
        """
        result_path = '../experiments/' + self._settings['experiment']['name'] + '/fairness/'
        os.makedirs(result_path, exist_ok=True)
        total_df = pd.DataFrame()
        for attribute in fairness_scores: # create one new plot per attribute
            for _, metric in enumerate(fairness_scores[attribute]): # one row per metric
                if metric not in ['equalised_odds']:
                    score_df = self._format_fairness(fairness_scores[attribute][metric])
                    score_df['demographic'] = score_df['demographic'].apply(lambda x: '{}'.format(x))
                    colours = self.get_colours(len(np.unique(score_df['demographic'])))

                    # HARDCODING EDM
                    # width + height
                    unique_demographics = len(score_df['demographic'].unique())
                    # colours
                    if self._settings['pipeline']['dataset'] == 'xuetangx' and attribute == 'gender':
                        score_df['demographic'] = score_df['demographic'].astype(str).replace('0', 'privileged').replace('1', 'privileged')
                        score_df['demographic'] = score_df['demographic'].replace('2', 'protected').replace('3', 'privileged')

                    if (self._settings['pipeline']['dataset'] == 'eedi' or self._settings['pipeline']['dataset'] == 'eedi2') and attribute == 'gender':
                        score_df['demographic'] = score_df['demographic'].astype(str).replace('0', 'privileged').replace('2', 'privileged')
                        score_df['demographic'] = score_df['demographic'].replace('1', 'protected').replace('3', 'protected')

                    if self._settings['pipeline']['dataset'] == 'oulad' and (attribute == 'gender' or attribute=='disability'):
                        score_df['demographic'] = score_df['demographic'].astype(str).replace('0', 'privileged').replace('1', 'protected')

                    if 'student-performance' in self._settings['pipeline']['dataset']  and attribute == 'sex':
                        score_df['demographic'] = score_df['demographic'].astype(str).replace('0', 'privileged').replace('1', 'protected')

                    score_df = score_df.sort_values('demographic')
                    colours = ['#E9D8A6', '#DCBE35', '#004346']

                    fig = px.box(
                        score_df, x='experiment', y='scores', color='demographic', color_discrete_sequence=colours, 
                        title='{} {} : {} {}'.format('*' * 20, attribute.upper(), metric, '*' * 20), width=200+unique_demographics*80, height=400
                    )
                    fig.update_layout(
                        plot_bgcolor='white'
                    )
                    fig.update_xaxes(
                        mirror=True,
                        ticks='outside',
                        showline=True,
                        linecolor='black',
                        gridcolor='white'
                    )
                    fig.update_yaxes(
                        mirror=True,
                        ticks='outside',
                        showline=True,
                        linecolor='white',
                        gridcolor='lightgrey',
                        rangemode='tozero',
                        range=[0,1]
                    )
                    fig.update_traces(quartilemethod="exclusive") # or "inclusive", or "linear" by default

                    if self._settings['save']:
                        fig.write_image('{}/fairness_att{}_{}.svg'.format(result_path, attribute, metric))
                    if self._settings['show']:
                        fig.show()
                    else:
                        fig = []
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        score_df = self._format_fairness(fairness_scores[attribute][metric][submetric])
                        score_df['demographic'] = score_df['demographic'].apply(lambda x: '{}'.format(x))
                        colours = self.get_colours(len(np.unique(score_df['demographic'])))
                        fig = px.box(
                            score_df, x='experiment', y='scores', color='demographic', color_discrete_sequence=colours, 
                            title='{} {} : {} - {} {}'.format('*' * 20, attribute.upper(), metric, submetric, '*' * 20), width=1200, height=700
                        )
                        fig.update_layout(
                            plot_bgcolor='white'
                        )
                        fig.update_xaxes(
                            mirror=True,
                            ticks='outside',
                            showline=True,
                            linecolor='black',
                            gridcolor='white'
                        )
                        fig.update_yaxes(
                            mirror=True,
                            ticks='outside',
                            showline=True,
                            linecolor='white',
                            gridcolor='lightgrey',
                            rangemode='tozero',
                            range=[0,1]
                        )
                        fig.update_traces(quartilemethod="exclusive") # or "inclusive", or "linear" by default

                        if self._settings['save']:
                            fig.write_image('{}/fairness_att{}_{}_{}.svg'.format(result_path, attribute, metric, submetric))
                        if self._settings['show']:
                            fig.show()
                        else:
                            fig = []

                score_df['metric'] = metric
                score_df['attribute'] = attribute
                total_df = pd.concat([total_df, score_df])

        total_df.to_csv('{}/fairness_df.csv'.format(result_path))
        mean_df = total_df .groupby(['metric', 'attribute', 'demographic', 'experiment']).mean()[['scores']]
        std_df = total_df .groupby(['metric', 'attribute', 'demographic', 'experiment']).std()[['scores']]
        mean_df = mean_df.rename(columns={'scores':'mean'})
        std_df = std_df.rename(columns={'scores':'std'})
        total_df = mean_df.merge(std_df, on=['metric', 'attribute', 'demographic', 'experiment'])
        total_df.to_csv('{}/mean_fairness_df.csv'.format(result_path))

    def _format_classification(self, classification_scores):
        """format the fairness scores dictionary for plotting mode
        """
        clf_df = []
        for metric in classification_scores:
            for experiment in classification_scores[metric]:
                cdf = pd.DataFrame.from_dict(classification_scores[metric][experiment]).reset_index()
                cdf = cdf.rename(columns={0:'scores', 'index': 'fold'})
                cdf['metric'] = metric
                cdf['experiment'] = experiment
                clf_df.append(cdf)
        clf_df = pd.concat(clf_df)
        clf_df.dropna(how='any')
        clf_df = clf_df[['scores', 'metric', 'experiment']]
        return clf_df

    def plot_classification(self, classification_scores):
        """creates classification plot
        """
        result_path = '../experiments/' + self._settings['experiment']['name'] + '/classification/'
        os.makedirs(result_path, exist_ok=True)
        score_df = self._format_classification(classification_scores)
        colours = self.get_colours(len(np.unique(score_df['experiment'])), 'baseline')
        unique_experiments = len(score_df['experiment'].unique())
        fig = px.box(
            score_df, x='metric', y='scores', color='experiment', color_discrete_sequence=colours, 
            title='{} Classification Results {}'.format('*' * 20, '*' * 20), width=200+unique_experiments*100, height=400
        )
        fig.update_layout(
            plot_bgcolor='white'
        )
        fig.update_xaxes(
            mirror=True,
            ticks='outside',
            showline=True,
            linecolor='black',
            gridcolor='white'
        )
        fig.update_yaxes(
            mirror=True,
            ticks='outside',
            showline=True,
            linecolor='white',
            gridcolor='lightgrey',
            rangemode='tozero',
            range=[0,1]
        )
        fig.update_traces(quartilemethod="exclusive") # or "inclusive", or "linear" by default

        score_df.to_csv('{}/score_df.csv'.format(result_path))
        mean_df = score_df.groupby(['metric', 'experiment']).mean()[['scores']]
        std_df = score_df.groupby(['metric', 'experiment']).std()[['scores']]
        mean_df = mean_df.rename(columns={'scores':'mean'})
        std_df = std_df.rename(columns={'scores':'std'})
        total_df = mean_df.merge(std_df, on=['metric', 'experiment'])
        total_df.to_csv('{}/mean_fairness_df.csv'.format(result_path))
        mean_df.to_csv('{}/mean_scores_df.csv'.format(result_path))

        if self._settings['save']:
            fig.write_image('{}/classification.svg'.format(result_path))
        if self._settings['show']:
            fig.show()
        else:
            fig = []

    def plot(self, features, labels, demographics):
        results = self._crawl()

        fairness_scores = {}
        classification_scores = {}
        for experiment_name in results:
            print('Collecting: {}'.format(experiment_name))
            try:
                fs, cs = self.extract_metrics_perfold(results[experiment_name]['data'], labels, demographics)
            except KeyError:
                fs, cs = self.extract_metrics_bootstrap(results[experiment_name]['data'], labels, demographics)
            fairness_scores[experiment_name] = fs
            classification_scores[experiment_name] = cs
        fairness_scores, classification_scores = self._merge_experiment_scores(fairness_scores, classification_scores)

        self.plot_fairness(fairness_scores)
        self.plot_classification(classification_scores)

# Significance
    def significance_across_demographics(self, experiment, predictions, labels, demographics):
        """Computes all metrics as a bootstrapping over all predictions over all folds

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        self._nfolds = self._settings['n_bootstraps']
        
        # Format fairness scores
        binary_scorer = BinaryClfScorer(self._settings)
        significances = []
        significance_details = ['attribute', 'metric', 'demo_a', 'demo_b', 'p-value']
        for attribute in self._settings['fairness']['attributes']:
            att_demographics = [dem[attribute] for dem in demographics]

            # Compute original metric for sample a
            for i_a in range(len(self._fairness_demo_map[attribute])):
                dem_a = self._fairness_demo_map[attribute][i_a]
                sample_a = [i for i in range(len(demographics)) if att_demographics[i] == dem_a]
                n_a = len(sample_a)
                # Compute original metric for sample b
                for i_b in range(i_a+1, len(self._fairness_demo_map[attribute])):
                    dem_b = self._fairness_demo_map[attribute][i_b]
                    sample_b = [i for i in range(len(demographics)) if att_demographics[i] == dem_b]
                    n_b = len(sample_b)

                    # Create Bootstraps
                    n = n_a + n_b
                    all_samples = [*sample_a, *sample_b]

                    for metric in ['balanced_accuracy', 'precision', 'recall', 'roc', 'fp', 'tp', 'fn']:
                        original_a = binary_scorer.get_score(metric, sample_a, labels, predictions['preds'], predictions['probas'], att_demographics)
                        original_b = binary_scorer.get_score(metric, sample_b, labels, predictions['preds'], predictions['probas'], att_demographics)
                        original_diff = np.abs(original_a - original_b)

                        sup_metric = 0
                        for _ in range(self._settings['n_bootstraps']):
                            np.random.shuffle(all_samples)
                            bootstrap_a = all_samples[:n_a]
                            bootstrap_b = all_samples[n_a:]
                            assert len(bootstrap_a) == len(sample_a) and len(bootstrap_b) == len(sample_b)
                            bootstrap_diff = np.abs(
                                binary_scorer.get_score(metric, bootstrap_a, labels, predictions['preds'], predictions['probas'], att_demographics) - \
                                    binary_scorer.get_score(metric, bootstrap_b, labels, predictions['preds'], predictions['probas'], att_demographics)
                            )
                            if bootstrap_diff >= original_diff:
                                sup_metric += 1
                        fischer = sup_metric / self._settings['n_bootstraps']
                        significances.append([
                            attribute, metric, dem_a, dem_b, fischer
                        ])
            fischers = pd.DataFrame(significances)
            fischers.columns = significance_details

            result_path = '../experiments/{}'.format(self._settings['experiment']['name'])
            fischers.to_csv('{}/{}/fischers_across_demographics.csv'.format(result_path, experiment))

    def significance_across_experiments(self, predictions, labels):
        result_path = '../experiments/{}'.format(self._settings['experiment']['name'])
        experiments = [k for k in predictions.keys()]
        binary_scorer = BinaryClfScorer(self._settings)

        significances = []
        significance_details = ['metric', 'experiment_a', 'experiment_b', 'p-value']
        for e_a in range(len(experiments)):
            experiment_a = experiments[e_a]
            n_a = len(predictions[experiment_a]['preds'])
            for e_b in range(e_a+1, len(experiments)):
                experiment_b = experiments[e_b]
                n_b = len(predictions[experiment_b]['preds'])

                n = n_a + n_b
                all_samples = [i for i in range(n)]
                all_labels = [*labels, *labels]
                all_predictions = [*predictions[experiment_a]['preds'], *predictions[experiment_b]['preds']]
                all_probas = [*predictions[experiment_a]['probas'], *predictions[experiment_b]['probas']]

                for metric in ['balanced_accuracy', 'precision', 'recall', 'roc', 'fp', 'tp', 'fn', 'rmse']:
                    original_a = binary_scorer.get_score(
                        metric, 
                        [i for i in range(n_a)],
                        labels, predictions[experiment_a]['preds'], predictions[experiment_a]['probas'], []
                    )
                    original_b = binary_scorer.get_score(
                        metric,
                        [i for i in range(n_b)],
                        labels, predictions[experiment_b]['preds'], predictions[experiment_b]['probas'], []
                    )
                    original_diff = np.abs(original_a - original_b)

                    sup_metric = 0
                    for _ in range(self._settings['n_bootstraps']):
                        np.random.shuffle(all_samples)
                        bootstrap_a = all_samples[:n_a]
                        bootstrap_b = all_samples[n_a:]
                        assert len(bootstrap_a) == len(predictions[experiment_a]['probas']) and len(bootstrap_b) == len(predictions[experiment_b]['probas'])
                        bootstrap_diff =  np.abs(
                            binary_scorer.get_score(metric, bootstrap_a, all_labels, all_predictions, all_probas, []) - \
                                binary_scorer.get_score(metric, bootstrap_b, all_labels, all_predictions, all_probas, [])
                        )
                        if bootstrap_diff >= original_diff:
                            sup_metric += 1
                    fischer = sup_metric / self._settings['n_bootstraps']
                    significances.append([metric, experiment_a, experiment_b, fischer])
                fischers = pd.DataFrame(significances)
                fischers.columns = significance_details

            result_path = '../experiments/{}'.format(self._settings['experiment']['name'])
            fischers.to_csv('{}/fischers_across_experiments.csv'.format(result_path))

    def compare(self, features, labels, demographics):
        results = self._crawl()
        self._fairness_demo_map = {
            att: [dem for dem in np.unique([d[att] for d in demographics])] for att in self._settings['fairness']['attributes']
        }
        predictions = {}
        for experiment_name in results:
            predictions[experiment_name] = self.extract_predictions(results[experiment_name]['data'], labels, demographics)

            self.significance_across_demographics(experiment_name, predictions[experiment_name], labels, demographics)
        self.significance_across_experiments(predictions, labels)
        


        

        




























# fig = make_subplots(
#     rows=len(fairness_scores[attribute].keys()),
#     # row_heights=[4000 for _ in range(len(fairness_scores[attribute].keys()))]
# )
# fig.update_layout(height=10000)
# figures = []

# mrow = 1
# # for _, metric in enumerate(fairness_scores[attribute]): # one row per metric
# #     # if metric not in ['equalised_odds', 'equal_opportunity', 'demographic_parity']:
# #     if metric in ['tp', 'fp', 'fn']:
# #         score_df = self._format_fairness(fairness_scores[attribute][metric])
# #         score_df['demographic'] = score_df['demographic'].apply(lambda x: '{}_{}'.format(metric, x))
# #         colours = self.get_colours(len(np.unique(score_df['demographic'])))
# #         figures.append(
# #             px.box(score_df, x='experiment', y='scores', color='demographic', color_discrete_sequence=colours)#, width=800, height=40000)
# #         )
# #         for trace in range(len(figures[-1]['data'])):
# #             fig.append_trace(figures[-1]['data'][trace], row=mrow, col=1)
# #         mrow += 1
        
# # fig.show()
# # # plot(dcc.Graph(figure=fig))
# # # plot(final_graph)
# # break