
import os
from random import sample
import re
import pickle
import numpy as np
import pandas as pd

import plotly.express as px
from matplotlib import pyplot as plt
from crossvalidation.scorers.binary_scorer import BinaryClfScorer
from crossvalidation.scorers.fairness_binary_scorer import BinaryFairnessScorer

from plotters.report import Report
from collections import Counter

# import dash_core_components as dcc
from plotly.offline import plot
from plotly.subplots import make_subplots
import plotly.graph_objects as go

class StatisticsReport(Report):
    def __init__(self, settings:dict):
        self._settings = dict(settings)
        self.get_reporting_path('data_reports')
        self._colours = [
            '#0C1618', '#004346', '#3F7267', '#7D9D8B', '#FAF4D3', '#E6D06A', '#DCBE35', '#D1AC00', '#7389AE',
            '#001219', '#005F73', '#0A9396', '#E9D8A6', '#EE9B00', '#CA6702', '#BB3E03', '#AE2012', '#9B2226'
        ]

# General
    def _crawl(self):
        # crawl paths
        xval_path = []
        experiment_path = '../experiments/' + self._settings['experiment']['name'] + '/'
        for (dirpath, _, filenames) in os.walk(experiment_path):
            files = [os.path.join(dirpath, file) for file in filenames]
            xval_path.extend(files)
        xval_path = [xval for xval in xval_path if '/results/' in xval and 'config.pkl' not in xval and '.csv' not in xval]
        xval_path = [xval for xval in xval_path if 'exclude' not in xval and '.DS_Store' not in xval]
        
        # Load results
        key_re = re.compile('{}[A-z\_\-0-9/]*mitigation([A-z\_\-0-9/]*)/2025'.format(experiment_path))
        xvs = {}
        for xv in xval_path:
            if 'baseline' in xv:
                curr_key = 'baseline'
            else:
                key = key_re.findall(xv)
                print('processing: {} ({}) '.format(xv, key))
                assert len(key) == 1
                curr_key = '/'.join(key[0].split('/')[:self._settings['plotter']['experiment_indices']])
                curr_key = curr_key.replace('in', '').replace('post', '').replace('pre', '')
            xvs[curr_key] = {}

            # cross validation
            with open(xv, 'rb') as fp:
                xvs[curr_key]['data'] = pickle.load(fp)

            # config file
            conpath = '{}/results/config.pkl'.format(xv.split('/results/')[0])
            with open(conpath, 'rb') as fp:
                xvs[curr_key]['config'] = pickle.load(fp)

            # result path
            xvs[curr_key]['result_path'] = conpath.replace('config.pkl', 'overall_results/')
            self._result_path = conpath.replace('config.pkl', '/statistics/')
            os.makedirs(xvs[curr_key]['result_path'], exist_ok=True)
            os.makedirs(self._result_path, exist_ok=True)

        return xvs

    def extract_metrics_perfold(self, experiment, labels, demographics):
        """Computes all metrics, for each fold

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        self._nfolds = 10

        # Collect scores
        binary_scorer = BinaryClfScorer(self._settings)
        fairness_scorer = BinaryFairnessScorer(self._settings)
        scores = {}
        for fold in range(10):
            truths = [labels[tidx] for tidx in experiment[fold]['test_index']]
            probas = experiment[fold]['y_proba']
            preds = experiment[fold]['y_pred']
            demos = [demographics[tidx] for tidx in experiment[fold]['test_index']] 
            scores[fold] =  {
                'classification': binary_scorer.get_scores(truths, preds, probas, demos),
                'fairness': fairness_scorer.get_fairness_scores(truths, preds, probas, demos)
            }
            
        # Format fairness scores
        fairness_scores = {}
        for attribute in self._settings['fairness']['attributes']:
            fairness_scores[attribute] = {}
            for fairness_metric in fairness_scorer.get_metrics():
                fairness_scores[attribute][fairness_metric] = {}
                if fairness_metric != 'equalised_odds':
                    for demographic_attribute in scores[0]['fairness'][attribute][fairness_metric]:
                        fairness_scores[attribute][fairness_metric][demographic_attribute] = []
                        for fold in range(self._nfolds):
                            if demographic_attribute in scores[fold]['fairness'][attribute][fairness_metric]:
                                fairness_scores[attribute][fairness_metric][demographic_attribute].append(
                                    scores[fold]['fairness'][attribute][fairness_metric][demographic_attribute]
                                )
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        fairness_scores[attribute][fairness_metric][submetric] = {}
                        for demographic_attribute in scores[0]['fairness'][attribute][fairness_metric][submetric]:
                            fairness_scores[attribute][fairness_metric][submetric][demographic_attribute] = []
                            for fold in range(self._nfolds):
                                if demographic_attribute in scores[fold]['fairness'][attribute][fairness_metric][submetric]:
                                    fairness_scores[attribute][fairness_metric][submetric][demographic_attribute].append(
                                        scores[fold]['fairness'][attribute][fairness_metric][submetric][demographic_attribute]
                                    ) 

        # Format classification scores
        classification_scores = {}
        for clf_metric in self._settings['crossvalidation']['scorer']['scoring_metrics']:
            classification_scores[clf_metric] = []
            for fold in range(10):
                classification_scores[clf_metric].append(
                    scores[fold]['classification'][clf_metric]
                )

        return fairness_scores, classification_scores

    def extract_metrics_bootstrap(self, experiment, labels, demographics):
        """Computes all metrics as a bootstrapping over all predictions over all folds

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        self._nfolds = self._settings['n_bootstraps']
        
        # Prepare the data
        truths = []
        probas = []
        preds = []
        demos = []
        for fold in range(10):
            if fold in experiment:
                truths = [*truths, *[labels[tidx] for tidx in experiment[fold]['test_index']]]
                probas = [*probas, *experiment[fold]['y_proba']]
                preds = [*preds, *experiment[fold]['y_pred']]
                demos = [*demos, *[demographics[tidx] for tidx in experiment[fold]['test_index']]]
        assert len(truths) == len(probas) and len(probas) == len(preds) and len(preds) == len(demos)
        # Create the bootstrap sample
        binary_scorer = BinaryClfScorer(self._settings)
        fairness_scorer = BinaryFairnessScorer(self._settings)
        n_instances = len(truths)
        sample_indices = [i for i in range(n_instances)]
        fairness_boots = []
        binary_boots = []
        for _ in range(self._nfolds):
            boot_sample = np.random.choice(sample_indices, size=n_instances, replace=True)
            boot_truths = [truths[bidx] for bidx in boot_sample]
            boot_probas = [probas[bidx] for bidx in boot_sample]
            boot_preds = [preds[bidx] for bidx in boot_sample]
            boot_demos = [demos[bidx] for bidx in boot_sample]

            fairness_boots.append(fairness_scorer.get_fairness_scores(boot_truths, boot_preds, boot_probas, boot_demos))
            binary_boots.append(binary_scorer.get_scores(boot_truths, boot_preds, boot_probas, boot_demos))

        # Format fairness scores
        fairness_scores = {}
        for attribute in self._settings['fairness']['attributes']:
            fairness_scores[attribute] = {}
            for fairness_metric in fairness_scorer.get_metrics():
                fairness_scores[attribute][fairness_metric] = {}
                if fairness_metric != 'equalised_odds':
                    for demographic_attribute in fairness_boots[0][attribute][fairness_metric]:
                        fairness_scores[attribute][fairness_metric][demographic_attribute] = []
                        for fold in range(self._nfolds):
                            if demographic_attribute in fairness_boots[fold][attribute][fairness_metric]:
                                fairness_scores[attribute][fairness_metric][demographic_attribute].append(
                                    fairness_boots[fold][attribute][fairness_metric][demographic_attribute]
                                )
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        fairness_scores[attribute][fairness_metric][submetric] = {}
                        for demographic_attribute in fairness_boots[0][attribute][fairness_metric][submetric]:
                            fairness_scores[attribute][fairness_metric][submetric][demographic_attribute] = []
                            for fold in range(self._nfolds):
                                if demographic_attribute in fairness_boots[fold][attribute][fairness_metric][submetric]:
                                    fairness_scores[attribute][fairness_metric][submetric][demographic_attribute].append(
                                        fairness_boots[fold][attribute][fairness_metric][submetric][demographic_attribute]
                                    )

        # Format classification scores
        classification_scores = {}
        for clf_metric in self._settings['crossvalidation']['scorer']['scoring_metrics']:
            classification_scores[clf_metric] = []
            for fold in range(self._nfolds):
                classification_scores[clf_metric].append(
                    binary_boots[fold][clf_metric]
                )

        return fairness_scores, classification_scores

    def extract_predictions(self, experiment, labels, demographics):
        """Computes all metrics as a bootstrapping over all predictions over all folds

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        # Prepare the data
        truths = []
        probas = []
        preds = []
        demos = []
        for fold in range(10):
            if fold in experiment:
                truths = [*truths, *[labels[tidx] for tidx in experiment[fold]['test_index']]]
                probas = [*probas, *experiment[fold]['y_proba']]
                preds = [*preds, *experiment[fold]['y_pred']]
                demos = [*demos, *[demographics[tidx] for tidx in experiment[fold]['test_index']]]
        assert len(truths) == len(probas) and len(probas) == len(preds) and len(preds) == len(demos)
        return {'truths': truths, 'probas': probas, 'preds': preds, 'demos': demos}

    def _merge_experiment_scores(self, fairness_scores, classification_scores):
        """Merge the experiment metrics together

        Args:
            fairness_scores (dict): keys are the experiment, with fairness_scores as returned by extract_metrics
            classification_scores (dict): keys are the experiment, with the extract_metrics as returned by extract_metrics

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                    - experiment 
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        new_fairness_scores = {}
        new_classification_scores = {}
        experiment_names = [name for name in fairness_scores.keys()]

        for attribute in fairness_scores[experiment_names[0]]:
            new_fairness_scores[attribute] = {}
            for metric in fairness_scores[experiment_names[0]][attribute]:
                new_fairness_scores[attribute][metric] = {}
                if metric != 'equalised_odds':
                    for experiment in experiment_names:
                        new_fairness_scores[attribute][metric][experiment] = {}
                        for demographic in fairness_scores[experiment][attribute][metric]:
                            new_fairness_scores[attribute][metric][experiment][demographic] = fairness_scores[experiment][attribute][metric][demographic]
                else:
                    for submetric in ['tp', 'fp', 'max_diff']:
                        new_fairness_scores[attribute][metric][submetric] = {}
                        for experiment in experiment_names:
                            new_fairness_scores[attribute][metric][submetric][experiment] = {}
                            for demographic in fairness_scores[experiment][attribute][metric][submetric]:
                                new_fairness_scores[attribute][metric][submetric][experiment][demographic] = fairness_scores[experiment][attribute][metric][submetric][demographic]

        for metric in classification_scores[experiment_names[0]]:
            new_classification_scores[metric] = {}
            for experiment in experiment_names:
                new_classification_scores[metric][experiment] = classification_scores[experiment][metric]

        return new_fairness_scores, new_classification_scores

# Significance
    def significance_across_demographics(self, experiment, predictions, labels, demographics):
        """Computes all metrics as a bootstrapping over all predictions over all folds

        Args:
            experiment (dict): experiment file as generated by the pipeline
            labels (list): labels of the data as generated by the featurse pipeline
            demographics (list): demographics of the data as generated by the feature pipeline. Each instance is a dictionary where the key is the attribute, and the value is the demographic

        Returns:
            dict: multi level dict where the keys are:
            - attribute
                - metric
                        - demographic (value of the attribute)
                            - score values of all folds
        """
        self._nfolds = self._settings['n_bootstraps']
        
        # Format fairness scores
        binary_scorer = BinaryClfScorer(self._settings)
        significances = []
        significance_details = ['attribute', 'metric', 'demo_a', 'demo_b', 'p-value']
        for attribute in self._settings['fairness']['attributes']:
            att_demographics = [dem[attribute] for dem in demographics]

            # Compute original metric for sample a
            for i_a in range(len(self._fairness_demo_map[attribute])):
                dem_a = self._fairness_demo_map[attribute][i_a]
                sample_a = [i for i in range(len(demographics)) if att_demographics[i] == dem_a]
                n_a = len(sample_a)
                # Compute original metric for sample b
                for i_b in range(i_a+1, len(self._fairness_demo_map[attribute])):
                    dem_b = self._fairness_demo_map[attribute][i_b]
                    sample_b = [i for i in range(len(demographics)) if att_demographics[i] == dem_b]
                    n_b = len(sample_b)

                    # Create Bootstraps
                    n = n_a + n_b
                    all_samples = [*sample_a, *sample_b]
                    print(len(demographics), len(predictions), len(labels), len(predictions['preds']), len(predictions['probas']), len(att_demographics))
                    rolling_metrics = ['balanced_accuracy', 'precision', 'recall', 'roc', 'tp', 'fn']
                    if len(np.unique(labels)) > 2:
                        rolling_metrics = ['roc']
                    try:
                        for metric in rolling_metrics:
                            
                            print('metric', metric)
                            original_a = binary_scorer.get_score(metric, sample_a, predictions['truths'], predictions['preds'], predictions['probas'], predictions['demos'])
                            original_b = binary_scorer.get_score(metric, sample_b, predictions['truths'], predictions['preds'], predictions['probas'], predictions['demos'])
                            original_diff = np.abs(original_a - original_b)

                            sup_metric = 0
                            for _ in range(self._settings['n_bootstraps']):
                                np.random.shuffle(all_samples)
                                bootstrap_a = all_samples[:n_a]
                                bootstrap_b = all_samples[n_a:]
                                assert len(bootstrap_a) == len(sample_a) and len(bootstrap_b) == len(sample_b)
                                bootstrap_diff = np.abs(
                                    binary_scorer.get_score(metric, bootstrap_a, labels, predictions['preds'], predictions['probas'], att_demographics) - \
                                        binary_scorer.get_score(metric, bootstrap_b, labels, predictions['preds'], predictions['probas'], att_demographics)
                                )
                                if bootstrap_diff >= original_diff:
                                    sup_metric += 1
                            fischer = sup_metric / self._settings['n_bootstraps']
                            significances.append([
                                attribute, metric, dem_a, dem_b, fischer
                            ])
                    except ValueError:
                        continue
            fischers = pd.DataFrame(significances)
            fischers.columns = significance_details

            result_path = '../experiments/{}/statistics'.format(self._settings['experiment']['name'])
            os.makedirs(result_path, exist_ok=True)
            print('SAVING', '{}/{}_fischers_across_experiments_{}.csv'.format(result_path, experiment, attribute))
            fischers.to_csv('{}/{}_fischers_across_demographics_{}.csv'.format(result_path, experiment, attribute))

    def significance_across_experiments(self, predictions, labels):
        result_path = '../experiments/{}'.format(self._settings['experiment']['name'])
        experiments = [k for k in predictions.keys()]
        binary_scorer = BinaryClfScorer(self._settings)

        significances = []
        significance_details = ['metric', 'experiment_a', 'experiment_b', 'p-value']
        for e_a in range(len(experiments)):
            experiment_a = experiments[e_a]
            n_a = len(predictions[experiment_a]['preds'])
            for e_b in range(e_a+1, len(experiments)):
                experiment_b = experiments[e_b]
                n_b = len(predictions[experiment_b]['preds'])

                n = n_a + n_b
                all_samples = [i for i in range(n)]
                all_labels = [*labels, *labels]
                all_predictions = [*predictions[experiment_a]['preds'], *predictions[experiment_b]['preds']]
                all_probas = [*predictions[experiment_a]['probas'], *predictions[experiment_b]['probas']]

                for metric in ['balanced_accuracy', 'precision', 'recall', 'roc', 'fp', 'tp', 'fn', 'rmse']:
                    try:
                        original_a = binary_scorer.get_score(
                            metric, 
                            [i for i in range(n_a)],
                            labels, predictions[experiment_a]['preds'], predictions[experiment_a]['probas'], []
                        )
                        original_b = binary_scorer.get_score(
                            metric,
                            [i for i in range(n_b)],
                            labels, predictions[experiment_b]['preds'], predictions[experiment_b]['probas'], []
                        )
                        original_diff = np.abs(original_a - original_b)

                        sup_metric = 0
                        for _ in range(self._settings['n_bootstraps']):
                            np.random.shuffle(all_samples)
                            bootstrap_a = all_samples[:n_a]
                            bootstrap_b = all_samples[n_a:]
                            assert len(bootstrap_a) == len(predictions[experiment_a]['probas']) and len(bootstrap_b) == len(predictions[experiment_b]['probas'])
                            bootstrap_diff =  np.abs(
                                binary_scorer.get_score(metric, bootstrap_a, all_labels, all_predictions, all_probas, []) - \
                                    binary_scorer.get_score(metric, bootstrap_b, all_labels, all_predictions, all_probas, [])
                            )
                            if bootstrap_diff >= original_diff:
                                sup_metric += 1
                        fischer = sup_metric / self._settings['n_bootstraps']
                        significances.append([metric, experiment_a, experiment_b, fischer])
                    except ValueError:
                        continue
                fischers = pd.DataFrame(significances)
                fischers.columns = significance_details

            result_path = '../experiments/{}/statistics/'.format(self._settings['experiment']['name'])
            os.makedirs(result_path, exist_ok=True)
            
            fischers.to_csv('{}/fischers_across_experiments.csv'.format(result_path))

    def process_demographics(self, demographics):
        self._fairness_demo_map = {
            att: [dem for dem in np.unique([d[att] for d in demographics])] for att in self._settings['fairness']['attributes']
        }
        if len(self._settings['fairness']['attributes']) > 1:
            combined = '-'.join(self._settings['fairness']['attributes'])
            for demo in demographics:
                demo[combined] = '{}_{}'.format(self._settings['fairness']['attributes'][0], self._settings['fairness']['attributes'][1])

            self._fairness_demo_map[combined] = [dem for dem in np.unique([d[combined] for d in demographics])]

        return demographics

    def compare(self, features, labels, demographics):
        results = self._crawl()
        demographics = self.process_demographics(demographics)
        predictions = {}
        for experiment_name in results:
            predictions[experiment_name] = self.extract_predictions(results[experiment_name]['data'], labels, demographics)

            self.significance_across_demographics(experiment_name, predictions[experiment_name], predictions[experiment_name]['truths'], predictions[experiment_name]['demos'])
        if len(results.keys()) > 1:
            self.significance_across_experiments(predictions, labels)
        


        

        


























